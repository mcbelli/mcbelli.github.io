---
title: "Practical Machine Learning Project RMarkdown"
author: "Mike Belli"
date: "2023-06-30"
output: html_document
---
## Classification of Physical Exercise
  
```{r, echo=FALSE, message=FALSE, warning=FALSE}
#load packages
check.packages <- function(pkg){ 
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])] 
  if (length(new.pkg))  
    install.packages(new.pkg, dependencies = TRUE, repo="https://mirrors.nics.utk.edu/cran/") 
  sapply(pkg, require, character.only = TRUE) 
} 
packages<-c("sqldf","lattice","ggplot2"
            ,"lubridate","caret") 
check.packages(packages)

```



This project classifies how individuals did an exercise, using metrics from electronic devices worn that tracks movements, with classifications of A, B, C, D, and E. In this project, I will:
  1. read the testing and training data sets into R (from csv files), and create a "traintrain" data set and "traintest" data set out of the training data.
2. Use cross-fold validation on the "traintrain" data (with 10 groups) to find the best random forest model
3. Use the model to predict the testing data set, to get a view of what the out of sample accuracy should be for the testing data.

1. Read the data, create the "traintrain" and "traintest" data frames, and investigate
```{r}
train_data <- read.csv('pml-training.csv')
#local_file_test <- 'pml-testing.csv'
test_data <- read.csv('pml-testing.csv')
mydim <- dim(train_data)
set.seed(20230528)
train_data$var_num <- runif(dim(train_data)[1])
traintrain <- train_data[train_data$var_num<(.7),]; traintrain$var_num <- NULL
traintest <- train_data[train_data$var_num>=.7,] ; traintest$var_num <- NULL

listmiss <- sapply(traintrain, function(x) sum(is.na(x)))
listdf <- as.data.frame(listmiss)
listdf$var <- row.names(listdf)
listdf$num_missing <- listdf$listmiss
listdf <- listdf[c("var","num_missing")]
listdf2 <- listdf[listdf$num_missing>0,]
# there are exactly 13418 missing for all these. They represent descriptive summary statistics
# on the other variables, so exclude them from the analysis
traintrainb <- traintrain[c("classe","roll_belt","pitch_belt","yaw_belt","total_accel_belt")]
summary(traintrainb)

```

## Random Forest with Cross-Validation on "traintrain" data

Now perform random forest modeling with 10-fold cross-validation on the "traintrain" data, which is the "training" version of the training data set (randomly selected 70% of the training data).

```{r}
control <- trainControl(method='cv', number=10)
fit <- caret::train(classe ~ ., method="rf", trControl=control, data=traintrainb)
#summary(fit)
traintrainb$p_classe <- predict(fit,data=traintrainb)

check_acc <- as.data.frame(table(traintrainb$p_classe,traintrainb$classe))
names(check_acc)[names(check_acc)=='Var1'] <- 'p_classe'
names(check_acc)[names(check_acc)=='Var2'] <- 'classe'

sqldf('select classe,sum(Freq) as true_number
,sum(case when p_classe=classe then Freq else 0 end) as correct_number
,100*(sum(case when p_classe=classe then Freq else 0 end))/sum(Freq) as pct_correct
  from check_acc
      group by classe order by classe')
traintrainb$p_classe <- NULL

```

You can see that the percentage of predicted "classe" for the data was very high for the "traintrain" data (which is what was used to build the rf model).

Now let's look at how well the model predicts the "traintest" data, which was not part of the data used to create the model and is thus an out-of-sample test.
```{r}

traintestb <- traintest[c("classe","roll_belt","pitch_belt","yaw_belt","total_accel_belt")]
names(traintestb)
traintestb$p_classe <- predict(fit,newdata=traintestb)
table(traintestb$p_classe,traintestb$classe)
check_acc2 <- as.data.frame(table(traintestb$p_classe,traintestb$classe))
names(check_acc2)[names(check_acc2)=='Var1'] <- 'p_classe'
names(check_acc2)[names(check_acc2)=='Var2'] <- 'classe'
#ls(check_acc2)


sqldf('select classe,sum(Freq) as true_number
,sum(case when p_classe=classe then Freq else 0 end) as correct_number
,100*(sum(case when p_classe=classe then Freq else 0 end))/sum(Freq) as pct_correct
from check_acc2
group by classe order by classe'
      )
```


Finally, let's classify the test data set

```{r}

testb <- test_data[c("roll_belt","pitch_belt","yaw_belt","total_accel_belt")]
names(testb)
testb$p_classe <- predict(fit,newdata=testb)
testb
```

